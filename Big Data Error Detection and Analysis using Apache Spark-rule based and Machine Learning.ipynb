{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a53bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c277342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+---------+--------+--------------+-----+-----+--------------+---------+------+---------+------------+-------------+-----------+------------+----------+---------+----------+----------------+-------------------+\n",
      "|   f_name|    l_name|gender|area_code|   phone|          city|state|  zip|marital_status|has_child|salary|     rate|single_exemp|married_exemp|child_exemp|f_name_error|city_error|zip_error|rate_error|state_city_error|marital_child_error|\n",
      "+---------+----------+------+---------+--------+--------------+-----+-----+--------------+---------+------+---------+------------+-------------+-----------+------------+----------+---------+----------+----------------+-------------------+\n",
      "| Pengyuan|   Zendler|     F|      508|744-9007|    SWAMPSCOTT|   MA| 1907|             M|        N| 90000|      5.3|           0|         7150|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|      Nik|     Tacic|     M|      702|517-7658|     LAS VEGAS|   NV|89140|             M|        N| 90000|      0.0|           0|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|    Hovav|    Punter|     M|      501|304-9763|         HASTY|   AR|72640|             S|        N| 50000|      7.0|          20|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|Xiangning|  Vanneste|     F|      862|651-6469|    BRIGANTINE|   NJ| 8203|             M|        Y| 55000|1.9519792|           0|         2000|       1500|           0|         0|        0|         1|               0|                  0|\n",
      "|    Belen|    Niccum|     F|      920|287-1889|      FLORENCE|   WI|54121|             S|        Y| 85000|5.9232907|         700|            0|        400|           0|         0|        0|         1|               0|                  0|\n",
      "|Sudhanshu|    Meguro|     M|      915|996-7794|   BROWNSVILLE|   TX|78521|             S|        N| 60000|      0.0|           0|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|  Babette|   Abawajy|     M|      202|222-8737|    WASHINGTON|   DC|20226|             S|        N| 95000|      9.0|        1370|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|    Noury|   Deineko|     M|      785|107-5730|      LAWRENCE|   KS|66049|             S|        N| 10000|      3.5|        2250|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "| Rossella|    Chinen|     F|      202|868-7188|    WASHINGTON|   DC|20541|             S|        N| 35000|      9.0|        1370|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|   Fadila|   Mamelak|     F|      864|919-2348|SAINT MATTHEWS|   SC|29135|             M|        Y| 65000|      7.0|           0|         6600|       3300|           0|         0|        0|         1|               0|                  0|\n",
      "|    Nabuo|     Lugli|     F|      203|800-6915|     SOUTHBURY|   CT| 6488|             S|        N| 90000|      5.0|       12750|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|   Violet|     Broeg|     F|      406|353-7076|         BRADY|   MT|59416|             M|        N| 70000|      6.9|           0|         3800|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|  Elliott|  Mawatari|     M|      206|504-2865|        AUBURN|   WA|98001|             S|        Y| 85000|      0.0|           0|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|   Gherin|      Gori|     M|      312|722-5794|      ROCHELLE|   IL|61068|             S|        N| 85000|      3.0|        2000|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|   Thoddi|Obtulowicz|     M|      503|232-1410|     WILLAMINA|   OR|97396|             M|        N| 65000|      9.0|           0|          318|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|   Aurora|      Nogi|     M|      360|324-5693|      LAKEWOOD|   WA|98498|             S|        Y| 30000|      0.0|           0|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|     Irit| Smolensky|     F|      402|275-3579|        SENECA|   NE|69161|             M|        Y| 40000|     6.84|           0|          206|        103|           0|         0|        0|         1|               0|                  0|\n",
      "|     Tina|   Tsotras|     M|      843|589-8247|       GAFFNEY|   SC|29340|             S|        Y| 20000|      7.0|        3300|            0|       3300|           0|         0|        0|         1|               0|                  0|\n",
      "|   Howard|   Kageura|     M|      203|243-4810|      STAMFORD|   CT| 6913|             S|        N| 80000|      5.0|       12750|            0|          0|           0|         0|        0|         1|               0|                  0|\n",
      "|    Raven|     Prior|     F|      785|241-2136|      BEAUMONT|   KS|67012|             M|        N| 65000|     6.45|           0|         4500|          0|           0|         0|        0|         1|               0|                  0|\n",
      "+---------+----------+------+---------+--------+--------------+-----+-----+--------------+---------+------+---------+------------+-------------+-----------+------------+----------+---------+----------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, regexp_replace\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ErrorDetection\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df_dirty = spark.read.csv('tax_dirty.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Check for typos in the 'f_name' and 'city' columns\n",
    "df_dirty = df_dirty.withColumn('f_name_error', when(col('f_name').rlike('[^a-zA-Z\\']'), 1).otherwise(0))\n",
    "df_dirty = df_dirty.withColumn('city_error', when(col('city').rlike('[^a-zA-Z ]'), 1).otherwise(0))\n",
    "\n",
    "# Check for formatting issues in the 'zip' and 'rates' columns\n",
    "df_dirty = df_dirty.withColumn('zip_error', when(col('zip').rlike('^0|[^0-9]'), 1).otherwise(0))\n",
    "df_dirty = df_dirty.withColumn('rate_error', when(col('rate').contains('.'), 1).otherwise(0))\n",
    "\n",
    "# Check for violated attribute dependencies between 'state'/'city' and 'marital_status'/'has_child'\n",
    "df_dirty = df_dirty.withColumn('state_city_error', when((col('state') == 'CA') & (col('city') != 'San Francisco'), 1).otherwise(0))\n",
    "df_dirty = df_dirty.withColumn('marital_child_error', when((col('marital_status') == 'married') & (col('has_child') != 'yes'), 1).otherwise(0))\n",
    "\n",
    "df_dirty.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e41cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many errors in each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef142d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_name_error: 0\n",
      "city_error: 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip_error: 0\n",
      "rate_error: 199813\n",
      "state_city_error: 3831\n",
      "marital_child_error: 0\n"
     ]
    }
   ],
   "source": [
    "# Count the errors\n",
    "error_columns = ['f_name_error', 'city_error', 'zip_error', 'rate_error', 'state_city_error', 'marital_child_error']\n",
    "for column in error_columns:\n",
    "    print(f\"{column}: {df_dirty.filter(col(column) == 1).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "090b18ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5496733545778227\n",
      "+---------+----------+\n",
      "|   f_name|prediction|\n",
      "+---------+----------+\n",
      "| Pengyuan|       1.0|\n",
      "|      Nik|       1.0|\n",
      "|    Hovav|       1.0|\n",
      "|Xiangning|       1.0|\n",
      "|    Belen|       1.0|\n",
      "|Sudhanshu|       1.0|\n",
      "|  Babette|       1.0|\n",
      "|    Noury|       1.0|\n",
      "| Rossella|       1.0|\n",
      "|   Fadila|       1.0|\n",
      "|    Nabuo|       1.0|\n",
      "|   Violet|       1.0|\n",
      "|  Elliott|       1.0|\n",
      "|   Gherin|       1.0|\n",
      "|   Thoddi|       1.0|\n",
      "|   Aurora|       1.0|\n",
      "|     Irit|       1.0|\n",
      "|     Tina|       1.0|\n",
      "|   Howard|       1.0|\n",
      "|    Raven|       1.0|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, length\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ErrorDetection\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df_dirty = spark.read.csv('tax_dirty.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Prepare the data\n",
    "df_clean = df_dirty.drop('l_name', 'gender', 'area_code', 'phone', 'salary', 'single_exemp',\n",
    "                         'married_exemp', 'child_exemp')  # Drop unnecessary columns\n",
    "\n",
    "df_clean = df_clean.withColumn('error', when((col('f_name').rlike('[^a-zA-Z\\']')) | (col('city').rlike('[^a-zA-Z ]'))\n",
    "                                             | (col('zip').rlike('^0|[^0-9]')) | (col('rate').contains('.')) |\n",
    "                                             ((col('state') == 'CA') & (col('city') != 'San Francisco')) |\n",
    "                                             ((col('marital_status') == 'married') & (col('has_child') != 'yes')),\n",
    "                                             1).otherwise(0))\n",
    "\n",
    "# Feature engineering (example: extracting the length of 'f_name' and 'city' columns)\n",
    "df_clean = df_clean.withColumn('f_name_length', length(col('f_name').cast('string')))\n",
    "df_clean = df_clean.withColumn('city_length', length(col('city').cast('string')))\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "(train_data, test_data) = df_clean.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Prepare the features vector\n",
    "feature_columns = ['f_name_length', 'city_length']  # Add more columns as needed\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "# Train the model\n",
    "lr = LogisticRegression(labelCol='error', featuresCol='features')\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='error')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Predict errors on new data\n",
    "new_data = spark.read.csv('new_data.csv', inferSchema=True, header=True)  # Replace with your new data file\n",
    "\n",
    "# Feature engineering for new data (calculate f_name_length and city_length)\n",
    "new_data = new_data.withColumn('f_name_length', length(col('f_name').cast('string')))\n",
    "new_data = new_data.withColumn('city_length', length(col('city').cast('string')))\n",
    "\n",
    "new_data = assembler.transform(new_data)\n",
    "\n",
    "predictions_new_data = model.transform(new_data)\n",
    "predicted_errors = predictions_new_data.select('f_name', 'prediction').filter(col('prediction') == 1)\n",
    "\n",
    "predicted_errors.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a45611b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9081510151917853\n",
      "+---------+--------------+-----+----------+\n",
      "|   f_name|          city|  zip|prediction|\n",
      "+---------+--------------+-----+----------+\n",
      "| Pengyuan|    SWAMPSCOTT| 1907|       1.0|\n",
      "|      Nik|     LAS VEGAS|89140|       1.0|\n",
      "|    Hovav|         HASTY|72640|       1.0|\n",
      "|Xiangning|    BRIGANTINE| 8203|       1.0|\n",
      "|    Belen|      FLORENCE|54121|       1.0|\n",
      "|Sudhanshu|   BROWNSVILLE|78521|       1.0|\n",
      "|  Babette|    WASHINGTON|20226|       1.0|\n",
      "|    Noury|      LAWRENCE|66049|       1.0|\n",
      "| Rossella|    WASHINGTON|20541|       1.0|\n",
      "|   Fadila|SAINT MATTHEWS|29135|       1.0|\n",
      "|    Nabuo|     SOUTHBURY| 6488|       1.0|\n",
      "|   Violet|         BRADY|59416|       1.0|\n",
      "|  Elliott|        AUBURN|98001|       1.0|\n",
      "|   Gherin|      ROCHELLE|61068|       1.0|\n",
      "|   Thoddi|     WILLAMINA|97396|       1.0|\n",
      "|   Aurora|      LAKEWOOD|98498|       1.0|\n",
      "|     Irit|        SENECA|69161|       1.0|\n",
      "|     Tina|       GAFFNEY|29340|       1.0|\n",
      "|   Howard|      STAMFORD| 6913|       1.0|\n",
      "|    Raven|      BEAUMONT|67012|       1.0|\n",
      "+---------+--------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, length\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ErrorDetection\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df_dirty = spark.read.csv('tax_dirty.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Prepare the data\n",
    "df_clean = df_dirty.drop('l_name', 'gender', 'area_code', 'phone', 'salary', 'single_exemp',\n",
    "                         'married_exemp', 'child_exemp')  # Drop unnecessary columns\n",
    "\n",
    "df_clean = df_clean.withColumn('error', when((col('f_name').rlike('[^a-zA-Z\\']')) | (col('city').rlike('[^a-zA-Z ]'))\n",
    "                                             | (col('zip').rlike('^0|[^0-9]')) | (col('rate').contains('.')) |\n",
    "                                             ((col('state') == 'CA') & (col('city') != 'San Francisco')) |\n",
    "                                             ((col('marital_status') == 'married') & (col('has_child') != 'yes')),\n",
    "                                             1).otherwise(0))\n",
    "\n",
    "# Feature engineering (example: extracting the length of 'f_name', 'city', and 'zip' columns)\n",
    "df_clean = df_clean.withColumn('f_name_length', length(col('f_name').cast('string')))\n",
    "df_clean = df_clean.withColumn('city_length', length(col('city').cast('string')))\n",
    "df_clean = df_clean.withColumn('zip_length', length(col('zip').cast('string')))\n",
    "\n",
    "# Split the data into training and testing datasets\n",
    "(train_data, test_data) = df_clean.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Prepare the features vector\n",
    "feature_columns = ['f_name_length', 'city_length', 'zip_length']  # Add more columns as needed\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)\n",
    "\n",
    "# Train the model\n",
    "lr = LogisticRegression(labelCol='error', featuresCol='features')\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='error')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Predict errors on new data\n",
    "new_data = spark.read.csv('new_data.csv', inferSchema=True, header=True)  # Replace with your new data file\n",
    "\n",
    "# Feature engineering for new data (calculate f_name_length, city_length, and zip_length)\n",
    "new_data = new_data.withColumn('f_name_length', length(col('f_name').cast('string')))\n",
    "new_data = new_data.withColumn('city_length', length(col('city').cast('string')))\n",
    "new_data = new_data.withColumn('zip_length', length(col('zip').cast('string')))\n",
    "\n",
    "new_data = assembler.transform(new_data)\n",
    "\n",
    "predictions_new_data = model.transform(new_data)\n",
    "predicted_errors = predictions_new_data.select('f_name','city','zip', 'prediction').filter(col('prediction') == 1)\n",
    "\n",
    "predicted_errors.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
